# :brain: _CORTEXiRAG-BOT_
Este proyecto  consiste en un chatbot interactivo con interfaz en Streamlit capaz de interactuar con el usuario de manera casual o haciendo uso del beneficio de la t√©cnica RAG para proporcionar respuestas informadas basadas en el contenido de archivos PDF cargados por el usuario. La funcionalidad del modelo se logra gracias a librer√≠as como LangChain, LangSmith, PyPDFLoader, RecursiveCharacterTextSplitter y de embeddings.

## :file_folder: Estructura del c√≥digo
```
üìÅ LLM-Chatbot
|
‚îÇ‚îÄ‚îÄ üìÅ data
‚îÇ   |‚îÄ‚îÄ Manual de usuario.pdf                 # Archivos pdf para el RAG
‚îÇ   ‚îî‚îÄ‚îÄ Manual de usuario.pdf                 # Archivos pdf para el RAG
‚îÇ
‚îÇ‚îÄ‚îÄ üìÅ src
‚îÇ   |‚îÄ‚îÄ app.py        # Estructura principal del Streamlit y modelo LLM (C√≥digo principal)
‚îÇ   ‚îî‚îÄ‚îÄ rag.py        # Implementaci√≥n del RAG (M√≥dulo l√≥gica de la t√©cnica RAG)
|
‚îÇ‚îÄ‚îÄ .gitignore        # Archivos en .gitgnore
‚îÇ‚îÄ‚îÄ LICENSE           # Licensia MIT
‚îÇ‚îÄ‚îÄ README.md         # Archvio readme, descripci√≥n del proyecto
‚îÇ‚îÄ‚îÄ hello.py          # Archivo inicial al hacer uv init
‚îÇ‚îÄ‚îÄ pyproject.toml    # Apartado donde se encuentra la descripci√≥n del uv (versi√≥n python, librer√≠as instaladas)
‚îî‚îÄ‚îÄ uv.lock           # Lista de versiones de paquetes y dependencias para que el entorno sea reproducido sin problemas.
```

### :dart: Caracter√≠sticas
- Consulta inteligente de PDFs.
- Respuestas contextualizadas con Langchain, Ollama y t√©cnicas RAG.
- Procesamiento interno de PDFs y generaci√≥n de embeddings.
- Monitoreo y Trazabilidad con LangSmith del chatbot.
- Interfaz de usuario intuitiva con Streamlit.
- Configuraci√≥n de par√°metros para el modelo (temperature, top_p, etc).
- Si cuentas con modelos en Ollama, los extrae y te permite elegir con un selectbox, el modelo que gustes.

### :rocket: Tecnolog√≠as usadas
- Python
- uv
- Ollama
- Streamlit
- LangChain

### :framed_picture: Visualizaci√≥n de la aplicaci√≥n

| Vista Web | Vista M√≥vil |
|-----------|-------------|
| ![webapp](./data/) | ![mobile](./data/) |

| Vista Streamlit |
|------------------|
| ![st](./data/) |

---
## :hammer_and_wrench: Configuraci√≥n

### 1. Descargar Ollama y uv
- Ollama - [Download](https://ollama.com/)
- uv - [Terminal](https://docs.astral.sh/uv/#__tabbed_1_1)
> [!IMPORTANT]
> Para correr satisfactoriamente la aplicaci√≥n y configuraci√≥n debes contar con UV Python y Ollama (LLMs Open Source).

### 2. Instalar modelos de Ollama:
```bash
# Validar modelos de Ollama descargados
ollama list

# Descargar modelos qwen3
ollama run qwen3:latest
```

### 3. Clona el repositorio e instalar dependencias
```bash
git clone https://github.com/DonLuisM/LLM-Chatbot.git
cd LLM-Chatbot

uv sync
```

### 4. Ejecutar el streamlit para comparar respuestas
```bash
uv run streamlit run .\src\app.py 
```

### :scroll: Licencia
Licencia MIT ‚Äì consulta el archivo [LICENSE](./LICENSE) para m√°s detalles.

### :handshake: Contribuciones
Si deseas contribuir a este proyecto, si√©ntete libre de hacer un fork del repositorio y enviar un pull request. ¬°Todas las contribuciones son bienvenidas!

### :busts_in_silhouette: Autor:
- [@DonLuisM](https://github.com/DonLuisM)
